{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question Answering with Transformers\n",
        "\n",
        "End-to-end extractive Question Answering using SQuAD and BERT/DistilBERT.\n",
        "- Load the dataset\n",
        "- Preprocess and tokenize\n",
        "- Train / evaluate (Exact Match & F1)\n",
        "- Run inference: passage + question → answer span"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\AI_Diploma\\AI_diploma\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "ROOT = os.path.abspath(os.path.join(os.getcwd(), '..')) if os.path.basename(os.getcwd()) == 'src' else os.getcwd()\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "src_path = os.path.join(ROOT, 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "from data_loader import load_squad_data\n",
        "from preprocess import prepare_train_features, squad_metrics, normalize_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load SQuAD data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 87599\n",
            "Dev examples   : 10570\n",
            "\n",
            "Sample example:\n",
            "Context (first 200 chars): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta ...\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Answer  : Saint Bernadette Soubirous\n"
          ]
        }
      ],
      "source": [
        "data_dir = os.path.join(ROOT, 'data')\n",
        "train_examples, dev_examples = load_squad_data(data_dir=data_dir)\n",
        "\n",
        "print(f'Train examples: {len(train_examples)}')\n",
        "print(f'Dev examples   : {len(dev_examples)}')\n",
        "print('\\nSample example:')\n",
        "ex = train_examples[0]\n",
        "print('Context (first 200 chars):', ex['context'][:200], '...')\n",
        "print('Question:', ex['question'])\n",
        "print('Answer  :', ex['answers'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenizer and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Returned keys: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
            "Number of tokenized samples (with overflow): 2\n",
            "start_positions: [130, 52]\n",
            "end_positions  : [137, 56]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Inspect a small batch after preprocessing\n",
        "batch = train_examples[:2]\n",
        "out = prepare_train_features(batch, tokenizer, max_length=384, doc_stride=128)\n",
        "print('Returned keys:', list(out.keys()))\n",
        "print('Number of tokenized samples (with overflow):', len(out['input_ids']))\n",
        "print('start_positions:', out['start_positions'])\n",
        "print('end_positions  :', out['end_positions'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load model and train on a small subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 319.32it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
            "\u001b[1mDistilBertForQuestionAnswering LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_layer_norm.weight | UNEXPECTED | \n",
            "vocab_transform.bias    | UNEXPECTED | \n",
            "vocab_transform.weight  | UNEXPECTED | \n",
            "vocab_projector.bias    | UNEXPECTED | \n",
            "vocab_layer_norm.bias   | UNEXPECTED | \n",
            "qa_outputs.bias         | MISSING    | \n",
            "qa_outputs.weight       | MISSING    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "Tokenizing: 100%|██████████| 25/25 [00:00<00:00, 134.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size after tokenization: 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "from train import build_dataset, run_evaluation\n",
        "\n",
        "# Small subset for quick experimentation\n",
        "n_train, n_eval = 200, 50\n",
        "train_small = train_examples[:n_train]\n",
        "dev_small = dev_examples[:n_eval]\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "train_dataset = build_dataset(train_small, tokenizer, max_length=384, doc_stride=128, batch_size=8)\n",
        "\n",
        "print(f'Train dataset size after tokenization: {len(train_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
            "d:\\AI_Diploma\\AI_diploma\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/50 02:09 < 05:47, 0.10 it/s, Epoch 0.56/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.762893</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Train and save to the same folder that the Flask app expects: ROOT/outputs/final\n",
        "output_root = os.path.join(ROOT, 'outputs')\n",
        "os.makedirs(output_root, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_root,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch',\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "final_dir = os.path.join(output_root, 'final')\n",
        "trainer.save_model(final_dir)\n",
        "tokenizer.save_pretrained(final_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation: Exact Match and F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "metrics = run_evaluation(model, tokenizer, dev_small, device, max_length=384, doc_stride=128, batch_size=8)\n",
        "print('Exact Match:', metrics['exact_match'])\n",
        "print('F1 score   :', metrics['f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference: passage + question → answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from inference import QAInference\n",
        "\n",
        "model_path = os.path.join(output_root, 'final')\n",
        "qa = QAInference(model_path=model_path) if os.path.isdir(model_path) else QAInference(model_name=model_name)\n",
        "\n",
        "context = dev_examples[0]['context']\n",
        "question = dev_examples[0]['question']\n",
        "answer_text, score, start_char, end_char = qa.predict(question, context)\n",
        "\n",
        "print('Question     :', question)\n",
        "print('Predicted ans:', answer_text)\n",
        "print('Score        :', score)\n",
        "print('Ground truth :', dev_examples[0]['answers'][0]['text'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
